#!/usr/bin/env python3
"""
Generate Dataset for Cloud Training (Vertex AI)

This script generates both raw and processed data for model training,
uploads to GCS, and creates Vertex AI Managed Datasets.

Usage:
    # Generate dataset v1 and upload to GCS
    python scripts/05_deployment/generate_dataset.py --version v1
    
    # Use existing local data (skip generation, just upload)
    python scripts/05_deployment/generate_dataset.py --version v1 --use-existing
    
    # Specify custom GCS bucket
    python scripts/05_deployment/generate_dataset.py --version v1 --gcs-bucket my-custom-bucket
"""

import argparse
import os
import sys
import subprocess
import yaml
import importlib.util
from pathlib import Path
from datetime import datetime

# Dynamically import MultiTickerDataLoader (can't use standard import due to numeric prefix in '02_features')
data_loader_path = Path(__file__).parent.parent / '02_features' / 'tft' / 'tft_data_loader.py'
spec = importlib.util.spec_from_file_location('tft_data_loader', data_loader_path)
data_loader_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(data_loader_module)
MultiTickerDataLoader = data_loader_module.MultiTickerDataLoader


def generate_data(config_path: str, version: str, base_dir: str = 'data/datasets') -> dict:
    """
    Generate raw and processed data using unified logic.
    
    Args:
        config_path: Path to model config YAML
        version: Dataset version (e.g., 'v1', 'v2', 'nov2024')
        base_dir: Base directory for dataset storage
    
    Returns:
        Dictionary with paths to generated data
    """
    print("\n" + "="*80)
    print(f"   GENERATING DATASET: {version}")
    print("="*80)
    
    # Create version directory
    version_dir = Path(base_dir) / version
    raw_dir = version_dir / 'raw'
    processed_dir = version_dir / 'processed'
    
    raw_dir.mkdir(parents=True, exist_ok=True)
    processed_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nüìÇ Local directories:")
    print(f"   Raw: {raw_dir}")
    print(f"   Processed: {processed_dir}")
    
    # Step 1: Generate data using MultiTickerDataLoader pipeline
    print("\n" + "-"*80)
    print("STEP 1: Preparing data from BigQuery")
    print("-"*80)
    
    loader = MultiTickerDataLoader(config_path=config_path)
    splits = loader.prepare_data()
    # prepare_data() automatically exports to:
    #   - data/raw/tft_features.{csv,parquet}
    #   - data/processed/X_*.npy, y_*.npy, ts_*.npy (9 files for 3 splits)
    #   - data/processed/scalers.pkl
    #   - data/processed/metadata.yaml
    
    print("\n‚úÖ Data generated by data loader:")
    print(f"   Raw: data/raw/tft_features.{{csv,parquet}}")
    print(f"   Processed: data/processed/X_*.npy, y_*.npy, ts_*.npy")
    print(f"   Metadata: data/processed/{{scalers.pkl,metadata.yaml,feature_names.txt}}")
    
    # Step 2: Copy raw data to versioned directory
    print("\n" + "-"*80)
    print("STEP 2: Copying raw data to versioned directory")
    print("-"*80)
    
    import shutil
    from pathlib import Path as P
    
    # Copy from data/raw to versioned raw directory
    source_raw = P('data/raw')
    if source_raw.exists():
        for file in source_raw.glob('*'):
            if file.is_file():
                shutil.copy2(file, raw_dir / file.name)
                print(f"   ‚úÖ {file.name} ‚Üí {raw_dir / file.name}")
    
    # Step 3: Copy processed data to versioned directory
    print("\n" + "-"*80)
    print("STEP 3: Copying processed data to versioned directory")
    print("-"*80)
    
    # Copy from data/processed to versioned processed directory
    source_processed = P('data/processed')
    if source_processed.exists():
        for file in source_processed.glob('*'):
            if file.is_file():
                shutil.copy2(file, processed_dir / file.name)
                print(f"   ‚úÖ {file.name} ‚Üí {processed_dir / file.name}")
    
    print("\nüí° Note: Using numpy arrays (.npy) for training data")
    print("   Vertex AI Managed Datasets will use raw CSV for schema viewing")
    
    # Step 4: Create manifest
    manifest = {
        'version': version,
        'created': datetime.now().isoformat(),
        'config': config_path,
        'raw_dir': str(raw_dir),
        'processed_dir': str(processed_dir),
        'files': {
            'raw': [f.name for f in raw_dir.glob('*') if f.is_file()],
            'processed': [f.name for f in processed_dir.glob('*') if f.is_file()]
        },
        'data_stats': {
            'train_samples': splits['train'][0].shape[0],
            'val_samples': splits['val'][0].shape[0],
            'test_samples': splits['test'][0].shape[0],
            'num_features': splits['train'][0].shape[2],  # (samples, lookback, features)
            'lookback_window': splits['train'][0].shape[1],
            'prediction_horizons': splits['train'][1].shape[1]
        }
    }
    
    manifest_file = version_dir / 'manifest.yaml'
    with open(manifest_file, 'w') as f:
        yaml.dump(manifest, f, default_flow_style=False)
    
    print(f"\nüìÑ Manifest saved: {manifest_file}")
    
    print("\n" + "="*80)
    print(f"‚úÖ Dataset {version} generated successfully!")
    print("="*80)
    print(f"\nüìä Summary:")
    print(f"   Train: {manifest['data_stats']['train_samples']:,} sequences")
    print(f"   Val: {manifest['data_stats']['val_samples']:,} sequences")
    print(f"   Test: {manifest['data_stats']['test_samples']:,} sequences")
    print(f"   Features: {manifest['data_stats']['num_features']} (lookback: {manifest['data_stats']['lookback_window']})")
    print(f"   Horizons: {manifest['data_stats']['prediction_horizons']}")
    
    return manifest


def upload_to_gcs(version: str, gcs_bucket: str, base_dir: str = 'data/datasets') -> str:
    """
    Upload versioned dataset to GCS.
    
    Args:
        version: Dataset version
        gcs_bucket: GCS bucket name
        base_dir: Local base directory
    
    Returns:
        GCS base path for the version
    """
    print("\n" + "="*80)
    print(f"   UPLOADING TO GCS")
    print("="*80)
    
    version_dir = Path(base_dir) / version
    gcs_base_path = f"gs://{gcs_bucket}/datasets/{version}/"
    
    print(f"\n‚òÅÔ∏è  Destination: {gcs_base_path}")
    
    # Upload entire version directory (raw + processed + manifest)
    # Use rsync to exclude .gitkeep files (cp doesn't support -x)
    cmd = [
        'gsutil', '-m', 'rsync', '-r',
        '-x', r'.*/\.gitkeep$',  # Exclude .gitkeep files
        str(version_dir),
        gcs_base_path.rstrip('/')
    ]
    
    print(f"\nüöÄ Running: {' '.join(cmd)}")
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    if result.returncode != 0:
        print(f"‚ùå Upload failed: {result.stderr}")
        raise Exception(f"GCS upload failed: {result.stderr}")
    
    print(f"\n‚úÖ Upload complete!")
    print(f"   Raw data: {gcs_base_path}raw/")
    print(f"   Processed data: {gcs_base_path}processed/")
    print(f"   Manifest: {gcs_base_path}manifest.yaml")
    
    # Verify upload by listing files on GCS
    print(f"\nüîç Verifying GCS upload...")
    try:
        from google.cloud import storage
        client = storage.Client()
        bucket = client.bucket(gcs_bucket)
        
        prefix = f"datasets/{version}/"
        all_blobs = list(bucket.list_blobs(prefix=prefix))
        
        if all_blobs:
            print(f"   ‚úÖ Found {len(all_blobs)} files on GCS")
            
            # Define expected files
            expected_files = {
                'raw': ['tft_features.csv', 'tft_features.parquet'],
                'processed': [
                    'X_train.npy', 'y_train.npy', 'ts_train.npy',
                    'X_val.npy', 'y_val.npy', 'ts_val.npy',
                    'X_test.npy', 'y_test.npy', 'ts_test.npy',
                    'scalers.pkl', 'metadata.yaml', 'feature_names.txt'
                ],
                'root': ['manifest.yaml']
            }
            
            # Group by directory
            raw_files = [b for b in all_blobs if '/raw/' in b.name]
            processed_files = [b for b in all_blobs if '/processed/' in b.name]
            root_files = [b for b in all_blobs if b.name.count('/') == 2]
            
            validation_errors = []
            
            # Validate raw files
            if raw_files:
                print(f"\n   üìÇ raw/ ({len(raw_files)} files):")
                raw_names = set()
                for blob in sorted(raw_files, key=lambda b: b.name):
                    filename = blob.name.split('/')[-1]
                    raw_names.add(filename)
                    size_mb = blob.size / (1024 * 1024)
                    # .gitkeep files are expected to be empty
                    is_gitkeep = filename == '.gitkeep'
                    status = "‚úÖ" if (blob.size > 0 or is_gitkeep) else "‚ö†Ô∏è"
                    print(f"      {status} {filename:<35} {size_mb:>8.2f} MB")
                    if blob.size == 0 and not is_gitkeep:
                        validation_errors.append(f"raw/{filename} is empty (0 bytes)")
                
                # Check for missing expected files
                for expected in expected_files['raw']:
                    if expected not in raw_names:
                        validation_errors.append(f"Missing expected file: raw/{expected}")
            
            # Validate processed files
            if processed_files:
                print(f"\n   üìÇ processed/ ({len(processed_files)} files):")
                processed_names = set()
                for blob in sorted(processed_files, key=lambda b: b.name):
                    filename = blob.name.split('/')[-1]
                    processed_names.add(filename)
                    size_mb = blob.size / (1024 * 1024)
                    # .gitkeep files are expected to be empty
                    is_gitkeep = filename == '.gitkeep'
                    status = "‚úÖ" if (blob.size > 0 or is_gitkeep) else "‚ö†Ô∏è"
                    print(f"      {status} {filename:<35} {size_mb:>8.2f} MB")
                    if blob.size == 0 and not is_gitkeep:
                        validation_errors.append(f"processed/{filename} is empty (0 bytes)")
                
                # Check for missing expected files
                for expected in expected_files['processed']:
                    if expected not in processed_names:
                        validation_errors.append(f"Missing expected file: processed/{expected}")
            
            # Validate root files
            if root_files:
                print(f"\n   üìÇ root/ ({len(root_files)} files):")
                root_names = set()
                for blob in sorted(root_files, key=lambda b: b.name):
                    filename = blob.name.split('/')[-1]
                    root_names.add(filename)
                    size_kb = blob.size / 1024
                    status = "‚úÖ" if blob.size > 0 else "‚ö†Ô∏è"
                    print(f"      {status} {filename:<35} {size_kb:>8.2f} KB")
                    if blob.size == 0:
                        validation_errors.append(f"{filename} is empty (0 bytes)")
                
                # Check for missing expected files
                for expected in expected_files['root']:
                    if expected not in root_names:
                        validation_errors.append(f"Missing expected file: {expected}")
            
            total_size_mb = sum(b.size for b in all_blobs) / (1024 * 1024)
            print(f"\n   üìä Total size: {total_size_mb:.2f} MB")
            
            # Report validation results
            if validation_errors:
                print(f"\n   ‚ö†Ô∏è  Validation warnings ({len(validation_errors)}):")
                for error in validation_errors:
                    print(f"      ‚Ä¢ {error}")
                print(f"\n   ‚ö†Ô∏è  Upload may be incomplete or corrupted!")
            else:
                print(f"\n   ‚úÖ All expected files present and non-empty!")
        else:
            print(f"   ‚ö†Ô∏è  No files found on GCS at {gcs_base_path}")
            print(f"   This may indicate an upload failure.")
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Could not verify GCS upload: {e}")
    
    return gcs_base_path


def create_managed_dataset(version: str, gcs_bucket: str, project: str, region: str) -> dict:
    """
    Create Vertex AI Managed Datasets for raw and processed data.
    Uses TabularDataset with CSV files to enable schema viewing in Vertex AI Console.
    
    Args:
        version: Dataset version
        gcs_bucket: GCS bucket name
        project: GCP project ID
        region: GCP region
    
    Returns:
        Dictionary with dataset IDs
    """
    print(f"\n" + "="*80)
    print(f"   CREATING VERTEX AI MANAGED DATASETS")
    print("="*80)
    
    try:
        from google.cloud import aiplatform
    except ImportError:
        print("‚ö†Ô∏è  google-cloud-aiplatform not installed, skipping Managed Dataset creation")
        return {}
    
    aiplatform.init(project=project, location=region)
    
    gcs_base_path = f"gs://{gcs_bucket}/datasets/{version}/"
    
    # Helper function to find existing dataset by display name
    def find_existing_dataset(display_name: str):
        """Find an existing TabularDataset by display name."""
        try:
            datasets = aiplatform.TabularDataset.list(
                filter=f'display_name="{display_name}"',
                order_by='create_time desc'
            )
            if datasets:
                return datasets[0]  # Return most recent
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Could not check for existing dataset: {e}")
        return None
    
    # Create Raw Managed Dataset (CSV for schema analysis)
    raw_display_name = f"tft_raw_{version}"
    print(f"\nüîç Checking for existing raw dataset '{raw_display_name}'...")
    raw_dataset = find_existing_dataset(raw_display_name)
    
    if raw_dataset:
        print(f"   ‚ôªÔ∏è  Reusing existing dataset: {raw_dataset.resource_name}")
    else:
        print(f"\nüèóÔ∏è  Creating raw dataset (using CSV for schema)...")
        try:
            raw_dataset = aiplatform.TabularDataset.create(
                display_name=raw_display_name,
                gcs_source=f"{gcs_base_path}raw/tft_features.csv",
                labels={
                    'version': version.replace('.', '_'),  # Labels can't have dots
                    'type': 'raw',
                    'purpose': 'inspection'
                },
                sync=True
            )
            print(f"   ‚úÖ Raw Dataset: {raw_dataset.resource_name}")
            print(f"   üìä Schema: Viewable in Vertex AI Console")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Warning: Could not create raw dataset: {e}")
            raw_dataset = None
    
    # Note: No Managed Dataset for processed data (numpy arrays)
    # Training jobs load .npy files directly from GCS
    print(f"\nüí° Note: Processed data is in NumPy format (.npy)")
    print(f"   Training jobs load directly from: {gcs_base_path}processed/X_*.npy")
    print(f"   No Managed Dataset needed for numpy arrays")
    processed_dataset = None
    
    datasets = {
        'version': version,
        'raw_dataset_id': raw_dataset.resource_name if raw_dataset else 'N/A',
        'processed_dataset_id': processed_dataset.resource_name if processed_dataset else 'N/A',
        'gcs_base_path': gcs_base_path,
        'created': datetime.now().isoformat()
    }
    
    print(f"\nüí° Note: Managed Dataset (raw) uses CSV for schema viewing.")
    print(f"   Training jobs load NumPy arrays (.npy) directly - fastest format!")
    
    return datasets


def delete_dataset_version(version: str, gcs_bucket: str, project: str, region: str, 
                          delete_local: bool = True, registry_file: str = 'datasets_registry.yaml'):
    """
    Delete a dataset version from GCS, Vertex AI, local storage, and registry.
    
    Args:
        version: Dataset version to delete
        gcs_bucket: GCS bucket name
        project: GCP project ID
        region: GCP region
        delete_local: Whether to delete local files
        registry_file: Path to registry file
    """
    print(f"\n" + "="*80)
    print(f"   DELETING DATASET VERSION: {version}")
    print("="*80)
    
    # Load registry
    registry_path = Path(registry_file)
    registry = {}
    if registry_path.exists():
        with open(registry_path, 'r') as f:
            registry = yaml.safe_load(f) or {}
    
    if version not in registry:
        print(f"\n‚ö†Ô∏è  Version '{version}' not found in registry.")
        print(f"   Available versions: {', '.join(registry.keys()) if registry else 'None'}")
        return False
    
    version_info = registry[version]
    errors = []
    
    # Step 1: Delete Managed Datasets
    print(f"\nüóëÔ∏è  Step 1: Deleting Vertex AI Managed Datasets...")
    try:
        from google.cloud import aiplatform
        aiplatform.init(project=project, location=region)
        
        for ds_type in ['raw_dataset_id', 'processed_dataset_id']:
            if ds_type in version_info and version_info[ds_type] != 'N/A':
                try:
                    dataset = aiplatform.TabularDataset(version_info[ds_type])
                    dataset.delete()
                    print(f"   ‚úÖ Deleted {ds_type}: {version_info[ds_type]}")
                except Exception as e:
                    error_msg = f"Could not delete {ds_type}: {e}"
                    print(f"   ‚ö†Ô∏è  {error_msg}")
                    errors.append(error_msg)
    except ImportError:
        print("   ‚ö†Ô∏è  google-cloud-aiplatform not installed, skipping Managed Dataset deletion")
    except Exception as e:
        error_msg = f"Error deleting Managed Datasets: {e}"
        print(f"   ‚ö†Ô∏è  {error_msg}")
        errors.append(error_msg)
    
    # Step 2: Delete GCS files
    print(f"\nüóëÔ∏è  Step 2: Deleting GCS files...")
    try:
        from google.cloud import storage
        client = storage.Client(project=project)
        bucket = client.bucket(gcs_bucket)
        
        prefix = f"datasets/{version}/"
        blobs = list(bucket.list_blobs(prefix=prefix))
        
        if blobs:
            for blob in blobs:
                blob.delete()
            print(f"   ‚úÖ Deleted {len(blobs)} files from gs://{gcs_bucket}/{prefix}")
        else:
            print(f"   ‚ÑπÔ∏è  No files found at gs://{gcs_bucket}/{prefix}")
    except Exception as e:
        error_msg = f"Error deleting GCS files: {e}"
        print(f"   ‚ö†Ô∏è  {error_msg}")
        errors.append(error_msg)
    
    # Step 3: Delete local files
    if delete_local:
        print(f"\nüóëÔ∏è  Step 3: Deleting local files...")
        local_path = Path(version_info.get('local_path', f'data/datasets/{version}'))
        if local_path.exists():
            import shutil
            shutil.rmtree(local_path)
            print(f"   ‚úÖ Deleted local directory: {local_path}")
        else:
            print(f"   ‚ÑπÔ∏è  Local directory not found: {local_path}")
    else:
        print(f"\n‚è≠Ô∏è  Step 3: Skipping local file deletion (--keep-local)")
    
    # Step 4: Remove from registry
    print(f"\nüóëÔ∏è  Step 4: Removing from registry...")
    del registry[version]
    with open(registry_path, 'w') as f:
        yaml.dump(registry, f, default_flow_style=False, sort_keys=False)
    print(f"   ‚úÖ Removed '{version}' from {registry_path}")
    
    # Summary
    print(f"\n" + "="*80)
    if errors:
        print(f"‚ö†Ô∏è  Version '{version}' deleted WITH WARNINGS")
        print(f"\n‚ö†Ô∏è  Errors encountered:")
        for error in errors:
            print(f"   - {error}")
    else:
        print(f"‚úÖ Version '{version}' completely deleted!")
    print("="*80)
    
    remaining = list(registry.keys())
    if remaining:
        print(f"\nüìä Remaining versions: {', '.join(remaining)}")
    else:
        print(f"\nüìä No dataset versions remaining.")
    
    return len(errors) == 0


def save_dataset_registry(datasets: dict, registry_file: str = 'datasets_registry.yaml'):
    """
    Save or update dataset registry file.
    
    Args:
        datasets: Dataset information to save
        registry_file: Path to registry file
    """
    registry_path = Path(registry_file)
    
    # Load existing registry
    if registry_path.exists():
        with open(registry_path, 'r') as f:
            registry = yaml.safe_load(f) or {}
    else:
        registry = {}
    
    # Add or update version
    version = datasets['version']
    registry[version] = datasets
    
    # Save registry
    with open(registry_path, 'w') as f:
        yaml.dump(registry, f, default_flow_style=False, sort_keys=False)
    
    print(f"\nüíæ Dataset registry updated: {registry_path}")
    print(f"   Versions available: {', '.join(registry.keys())}")


def main():
    # Load environment variables from .env file
    from dotenv import load_dotenv
    project_root = Path(__file__).parent.parent.parent
    env_file = project_root / '.env'
    if env_file.exists():
        load_dotenv(env_file)
        print(f"‚úÖ Loaded environment from: {env_file}")
    else:
        print(f"‚ö†Ô∏è  Warning: .env file not found at {env_file}")
    
    parser = argparse.ArgumentParser(
        description='Generate TFT dataset for Vertex AI cloud training',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Generate v1 dataset and upload to GCS
  python scripts/05_deployment/generate_dataset.py --version v1
  
  # Use existing local data, just upload to GCS
  python scripts/05_deployment/generate_dataset.py --version v1 --use-existing
  
  # Specify custom GCS bucket
  python scripts/05_deployment/generate_dataset.py --version v2 --gcs-bucket my-bucket
  
  # Delete a single version
  python scripts/05_deployment/generate_dataset.py --delete-versions v1
  
  # Delete multiple versions
  python scripts/05_deployment/generate_dataset.py --delete-versions v1 v2 v3
        """
    )
    
    parser.add_argument('--version', type=str, required=False,
                       help='Dataset version (e.g., v1, v2, nov2024)')
    parser.add_argument('--delete-versions', type=str, nargs='+',
                       help='Delete specified dataset versions (e.g., --delete-versions v1 v2)')
    parser.add_argument('--config', type=str, default='configs/model_tft_config.yaml',
                       help='Path to model config YAML')
    parser.add_argument('--gcs-bucket', type=str, 
                       default=os.getenv('GCP_PROJECT_ID', 'your-project') + '-models',
                       help='GCS bucket name (default: {GCP_PROJECT_ID}-models)')
    parser.add_argument('--project', type=str, 
                       default=os.getenv('GCP_PROJECT_ID', 'your-project'),
                       help='GCP project ID (default: from GCP_PROJECT_ID env var)')
    parser.add_argument('--region', type=str, default='us-central1',
                       help='GCP region')
    parser.add_argument('--use-existing', action='store_true',
                       help='Skip data generation, use existing local data')
    parser.add_argument('--base-dir', type=str, default='data/datasets',
                       help='Base directory for dataset storage')
    
    args = parser.parse_args()
    
    # Handle deletion mode
    if args.delete_versions:
        print(f"\nüî¥ DELETION MODE: Deleting {len(args.delete_versions)} version(s)")
        print(f"   Versions to delete: {', '.join(args.delete_versions)}")
        
        confirm = input(f"\n‚ö†Ô∏è  This will permanently delete data from GCS, Vertex AI, and local storage.\n   Continue? [y/N]: ")
        if confirm.lower() != 'y':
            print("\n‚ùå Deletion cancelled.")
            return
        
        all_success = True
        for version in args.delete_versions:
            success = delete_dataset_version(
                version=version,
                gcs_bucket=args.gcs_bucket,
                project=args.project,
                region=args.region,
                delete_local=True
            )
            if not success:
                all_success = False
        
        if all_success:
            print(f"\n‚úÖ All versions deleted successfully!")
        else:
            print(f"\n‚ö†Ô∏è  Some versions had errors during deletion.")
        return
    
    # Generation mode requires --version
    if not args.version:
        parser.error("--version is required for dataset generation (or use --delete-versions to delete)")
    
    # Validate version naming
    poor_names = ['test', 'temp', 'tmp', 'debug', 'dev', 'latest', 'current']
    if args.version.lower() in poor_names:
        print(f"\n‚ö†Ô∏è  Warning: '{args.version}' is not a good version name.")
        print(f"   Recommended: Use semantic versions like 'v1', 'v2', or dates like '2024-11'")
        response = input(f"\n   Continue with '{args.version}' anyway? [y/N]: ")
        if response.lower() != 'y':
            print("\n‚ùå Aborted.")
            return
    
    # Check if version already exists (multiple locations)
    version_dir = Path(args.base_dir) / args.version
    registry_file = Path('datasets_registry.yaml')
    
    # Check local directory
    local_exists = version_dir.exists()
    
    # Check registry
    registry_exists = False
    registry = {}
    if registry_file.exists():
        with open(registry_file, 'r') as f:
            registry = yaml.safe_load(f) or {}
        registry_exists = args.version in registry
    
    # Check GCS (via registry)
    gcs_exists = registry_exists  # If in registry, it's in GCS
    
    # Handle conflicts
    if (local_exists or registry_exists) and not args.use_existing:
        print(f"\n‚ö†Ô∏è  Dataset version '{args.version}' already exists:")
        if local_exists:
            print(f"   üìÇ Local: {version_dir}")
        if registry_exists:
            print(f"   üìä Registry: datasets_registry.yaml")
        if gcs_exists:
            gcs_path = registry.get(args.version, {}).get('gcs_base_path', 'Unknown')
            print(f"   ‚òÅÔ∏è  GCS: {gcs_path}")
        
        print(f"\nüí° Options:")
        print(f"   1. Use existing data (recommended): --use-existing")
        print(f"   2. Create new version: --version v{int(args.version[1:])+1 if args.version.startswith('v') and args.version[1:].isdigit() else '2'}")
        print(f"   3. Overwrite {args.version} (destructive): Continue below")
        
        response = input(f"\n‚ö†Ô∏è  Overwrite version '{args.version}'? [y/N]: ")
        if response.lower() != 'y':
            print("\n‚ùå Aborted. No changes made.")
            return
        else:
            print(f"\n‚ö†Ô∏è  Proceeding with overwrite of version '{args.version}'...")
    
    try:
        # Step 1: Generate data locally
        if args.use_existing:
            print(f"\nüìÇ Using existing data at {version_dir}")
            manifest_file = version_dir / 'manifest.yaml'
            if not manifest_file.exists():
                raise FileNotFoundError(f"Manifest not found: {manifest_file}")
            with open(manifest_file, 'r') as f:
                manifest = yaml.safe_load(f)
        else:
            manifest = generate_data(args.config, args.version, args.base_dir)
        
        datasets_info = {
            'version': args.version,
            'local_path': str(version_dir),
            'created': manifest.get('created', datetime.now().isoformat())
        }
        
        # Step 2: Upload to GCS
        gcs_base_path = upload_to_gcs(args.version, args.gcs_bucket, args.base_dir)
        datasets_info['gcs_base_path'] = gcs_base_path
        
        # Step 3: Create Managed Datasets
        managed_datasets = create_managed_dataset(
            args.version,
            args.gcs_bucket,
            args.project,
            args.region
        )
        
        if managed_datasets:
            datasets_info.update(managed_datasets)
        
        # Step 4: Save to registry
        save_dataset_registry(datasets_info)
        
        # Final summary
        print("\n" + "="*80)
        print("   ‚ú® DATASET READY FOR CLOUD TRAINING!")
        print("="*80)
        print(f"\nüìÇ Local path: {version_dir}")
        print(f"‚òÅÔ∏è  GCS path: {datasets_info.get('gcs_base_path', 'N/A')}")
        print(f"\nüìä Managed Datasets:")
        print(f"   Raw: {datasets_info.get('raw_dataset_id', 'N/A')}")
        print(f"   Processed: {datasets_info.get('processed_dataset_id', 'N/A')}")
        
        print(f"\nüí° Next steps:")
        print(f"   # Single training job:")
        print(f"   python scripts/05_deployment/submit_job.py --dataset-version {args.version}")
        print(f"   ")
        print(f"   # Hyperparameter tuning (all trials share this dataset):")
        print(f"   python scripts/05_deployment/submit_hp_tuning.py --dataset-version {args.version}")
        
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()